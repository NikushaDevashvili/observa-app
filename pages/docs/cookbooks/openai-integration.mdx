# Cookbook: OpenAI Integration

Step-by-step guide to integrating Observa with OpenAI's API in JavaScript/TypeScript applications.

## Prerequisites

- Node.js 16+ or any JavaScript runtime
- OpenAI API key
- Observa API key (get from [Settings](https://observa-app.vercel.app/dashboard/settings))

## Set Up Environment

Install dependencies and set up environment variables:

```bash
npm install observa-sdk openai
```

```javascript
// .env
OPENAI_API_KEY=sk-proj-***
OBSERVA_API_KEY=your-observa-api-key
OBSERVA_API_URL=https://observa-api.vercel.app
```

## Example 1: Basic Chat Completion (Auto-Capture - Recommended)

Use `observeOpenAI()` for automatic tracking - the easiest way:

```javascript
import { init } from 'observa-sdk';
import OpenAI from 'openai';

// Initialize Observa SDK
const observa = init({
  apiKey: process.env.OBSERVA_API_KEY,
  apiUrl: process.env.OBSERVA_API_URL,
});

// Initialize OpenAI client
const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

// Wrap with Observa - automatic tracking!
const wrappedOpenAI = observa.observeOpenAI(openai, {
  name: 'my-app',
  userId: 'user-123',
});

// Use wrapped client - automatically tracked!
const response = await wrappedOpenAI.chat.completions.create({
  model: 'gpt-4',
  messages: [
    { role: 'user', content: 'What is the capital of France?' }
  ],
});

console.log(response.choices[0].message.content);
```

## Example 2: Streaming (Auto-Capture)

Streaming works automatically with `observeOpenAI()` - no extra code needed!

```javascript
// Wrap client once (same as Example 1)
const wrappedOpenAI = observa.observeOpenAI(openai, {
  name: 'my-app',
});

// Streaming automatically tracked!
const stream = await wrappedOpenAI.chat.completions.create({
  model: 'gpt-4',
  messages: [{ role: 'user', content: 'Tell me a joke' }],
  stream: true,
});

for await (const chunk of stream) {
  const content = chunk.choices[0]?.delta?.content || '';
  process.stdout.write(content);
}
```

## Example 3: RAG with Auto-Capture

Track RAG workflows with automatic capture:

```javascript
// Wrap client once
const wrappedOpenAI = observa.observeOpenAI(openai, {
  name: 'rag-app',
});

// Simulate retrieving context
const retrievedContext = "Paris is the capital and most populous city of France...";

// Use wrapped client - automatically tracked!
const response = await wrappedOpenAI.chat.completions.create({
  model: 'gpt-4',
  messages: [
    { 
      role: 'system', 
      content: `Use the following context to answer questions: ${retrievedContext}` 
    },
    { 
      role: 'user', 
      content: 'What is the capital of France?' 
    }
  ],
});

console.log(response.choices[0].message.content);
```

## Example 4: Multi-Turn Conversation

Track conversations with automatic capture:

```javascript
const wrappedOpenAI = observa.observeOpenAI(openai, {
  name: 'chat-app',
  userId: 'user-123',
});

const conversationId = `conv-${Date.now()}`;
const messages = [];

async function sendMessage(userMessage) {
  messages.push({ role: 'user', content: userMessage });
  
  // Use wrapped client - automatically tracked!
  const response = await wrappedOpenAI.chat.completions.create({
    model: 'gpt-4',
    messages: messages,
  });
  
  const assistantMessage = response.choices[0].message.content;
  messages.push({ role: 'assistant', content: assistantMessage });
  
  return assistantMessage;
}

// Use in conversation
await sendMessage("Hello!");
await sendMessage("What's the weather like?");
```

## PII Redaction

Use the `redact` option to sanitize sensitive data:

```javascript
const wrappedOpenAI = observa.observeOpenAI(openai, {
  name: 'secure-app',
  redact: (data) => {
    // Remove sensitive data before sending to Observa
    if (data?.messages) {
      return {
        ...data,
        messages: data.messages.map(msg => ({
          ...msg,
          content: '[REDACTED]'
        }))
      };
    }
    return data;
  },
});
```

## Best Practices

1. **Use Auto-Capture**: `observeOpenAI()` is the recommended approach - it automatically captures 90%+ of your LLM interactions
2. **Wrap Once**: Wrap your OpenAI client once and reuse it throughout your application
3. **Streaming Support**: Streaming works automatically - no extra code needed
4. **PII Redaction**: Use the `redact` option to sanitize sensitive data before sending to Observa
5. **Error Handling**: Wrap LLM calls in try-catch blocks

## Next Steps

- **[Next.js Integration](./nextjs-integration)**: Integrate with Next.js
- **[Multi-Turn Conversations](./multi-turn-conversations)**: Conversation patterns
- **[RAG Integration](./rag-integration)**: RAG workflows
- **[SDK Reference](../sdk/initialization)**: Complete API documentation
