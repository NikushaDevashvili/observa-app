# Cookbook: Express.js Integration

Step-by-step guide to integrating Observa with Express.js applications.

## Prerequisites

- Node.js 16+
- Express.js installed
- Observa API key (get from [Settings](https://observa-app.vercel.app/dashboard/settings))

## Set Up Environment

Install dependencies:

```bash
npm install observa-sdk express openai
```

Set up environment variables:

```bash
# .env
OBSERVA_API_KEY=your-observa-api-key
OBSERVA_API_URL=https://observa-api.vercel.app
OPENAI_API_KEY=sk-proj-***  # or your LLM provider key
PORT=3000
```

## Basic Setup

### 1. Initialize Observa

Create `lib/observa.js`:

```javascript
const { init } = require('observa-sdk');

module.exports = init({
  apiKey: process.env.OBSERVA_API_KEY,
  apiUrl: process.env.OBSERVA_API_URL || 'https://observa-api.vercel.app',
});
```

### 2. Basic Express Server (Auto-Capture)

Create `server.js`:

```javascript
require('dotenv').config();
const express = require('express');
const observa = require('./lib/observa');
const OpenAI = require('openai');

const app = express();
app.use(express.json());

// Initialize OpenAI client
const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

// Wrap with Observa - automatic tracking!
const wrappedOpenAI = observa.observeOpenAI(openai, {
  name: 'express-app',
});

app.post('/api/chat', async (req, res) => {
  try {
    const { message } = req.body;
    
    // Use wrapped client - automatically tracked!
    const response = await wrappedOpenAI.chat.completions.create({
      model: 'gpt-4',
      messages: [{ role: 'user', content: message }],
    });
    
    res.json({
      message: response.choices[0].message.content,
    });
  } catch (error) {
    console.error('Chat error:', error);
    res.status(500).json({ error: 'Failed to process request' });
  }
});

const PORT = process.env.PORT || 3000;
app.listen(PORT, () => {
  console.log(`Server running on port ${PORT}`);
});
```

## Advanced Patterns

### Conversation Management

Create a conversation handler:

```javascript
class ConversationManager {
  constructor() {
    this.conversations = new Map();
  }
  
  getConversationId(userId) {
    if (!this.conversations.has(userId)) {
      this.conversations.set(userId, `conv-${Date.now()}`);
    }
    return this.conversations.get(userId);
  }
}

const conversationManager = new ConversationManager();

// Wrap OpenAI client once (at module level)
const wrappedOpenAI = observa.observeOpenAI(openai, {
  name: 'express-chat-app',
});

app.post('/api/chat', async (req, res) => {
  try {
    const { message, userId } = req.body;
    const conversationId = conversationManager.getConversationId(userId);
    
    // Use wrapped client - automatically tracked!
    const response = await wrappedOpenAI.chat.completions.create({
      model: 'gpt-4',
      messages: [{ role: 'user', content: message }],
    });
    
    res.json({
      message: response.choices[0].message.content,
      conversationId: conversationId,
    });
  } catch (error) {
    console.error('Chat error:', error);
    res.status(500).json({ error: 'Failed to process request' });
  }
});
```

### Error Handling Middleware

Add error tracking with Observa:

```javascript
function errorHandler(err, req, res, next) {
  // Track error using Observa's trackError method
  observa.trackError({
    errorType: err.name || 'Error',
    errorMessage: err.message,
    stackTrace: err.stack,
    context: {
      path: req.path,
      method: req.method,
    },
  }).catch(console.error);
  
  res.status(err.status || 500).json({
    error: err.message || 'Internal server error',
  });
}

app.use(errorHandler);
```

## Complete Example

Full Express.js application with Observa auto-capture:

```javascript
require('dotenv').config();
const express = require('express');
const observa = require('./lib/observa');
const OpenAI = require('openai');

const app = express();
app.use(express.json());

// Initialize and wrap OpenAI client
const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

const wrappedOpenAI = observa.observeOpenAI(openai, {
  name: 'express-production-app',
  userId: 'server-user',
});

// Health check
app.get('/health', (req, res) => {
  res.json({ status: 'ok' });
});

// Chat endpoint
app.post('/api/chat', async (req, res) => {
  try {
    const { message, userId } = req.body;
    
    // Use wrapped client - automatically tracked!
    const response = await wrappedOpenAI.chat.completions.create({
      model: 'gpt-4',
      messages: [{ role: 'user', content: message }],
    });
    
    res.json({
      message: response.choices[0].message.content,
    });
  } catch (error) {
    console.error('Chat error:', error);
    res.status(500).json({ error: 'Failed to process request' });
  }
});

// Error handling
app.use((err, req, res, next) => {
  observa.trackError({
    errorType: err.name || 'Error',
    errorMessage: err.message,
    stackTrace: err.stack,
  }).catch(console.error);
  
  res.status(500).json({ error: 'Something went wrong!' });
});

const PORT = process.env.PORT || 3000;
app.listen(PORT, () => {
  console.log(`Server running on port ${PORT}`);
});
```

## Best Practices

1. **Use Auto-Capture**: Wrap your OpenAI/Anthropic client once with `observeOpenAI()` or `observeAnthropic()`
2. **Initialize Once**: Create a single Observa instance and reuse it
3. **Error Handling**: Use `trackError()` for error tracking
4. **Conversation Management**: Use consistent conversation IDs
5. **Environment Variables**: Never hardcode API keys

## Production Deployment

### Docker

```dockerfile
FROM node:18

WORKDIR /app

COPY package*.json ./
RUN npm install

COPY . .

ENV NODE_ENV=production
ENV PORT=3000

EXPOSE 3000

CMD ["node", "server.js"]
```

### Environment Variables

Set environment variables in your hosting platform:

- `OBSERVA_API_KEY`
- `OBSERVA_API_URL` (optional)
- `OPENAI_API_KEY` (or your LLM provider key)
- `PORT` (optional, defaults to 3000)

## Next Steps

- Check [OpenAI Integration](./openai-integration) for more examples
- Read [Multi-Turn Conversations](./multi-turn-conversations) for chat patterns
- See [SDK Reference](../sdk/initialization) for configuration options
- Review [Error Handling](../guides/error-handling) for best practices
