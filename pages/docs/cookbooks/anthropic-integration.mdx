# Cookbook: Anthropic Integration

Step-by-step guide to integrating Observa with Anthropic's Claude API.

## Prerequisites

- Node.js 16+ or any JavaScript runtime
- Anthropic API key
- Observa API key (get from [Settings](https://observa-app.vercel.app/dashboard/settings))

## Set Up Environment

Install dependencies and set up environment variables:

```bash
npm install observa-sdk @anthropic-ai/sdk
```

```javascript
// .env
ANTHROPIC_API_KEY=sk-ant-***
OBSERVA_API_KEY=your-observa-api-key
OBSERVA_API_URL=https://observa-api.vercel.app
```

## Example 1: Basic Message Completion

Simple example of tracking a Claude API call:

```javascript
import { init } from 'observa-sdk';
import Anthropic from '@anthropic-ai/sdk';

// Initialize Observa SDK
const observa = init({
  apiKey: process.env.OBSERVA_API_KEY,
  apiUrl: process.env.OBSERVA_API_URL,
});

// Initialize Anthropic client
const anthropic = new Anthropic({
  apiKey: process.env.ANTHROPIC_API_KEY,
});

// Track Claude call
const response = await observa.track(
  {
    query: "What is the capital of France?",
    conversationId: "conv-123",
    messageIndex: 1,
  },
  async () => {
    return await anthropic.messages.create({
      model: 'claude-3-opus-20240229',
      max_tokens: 1024,
      messages: [
        { role: 'user', content: 'What is the capital of France?' }
      ],
    });
  }
);

console.log(response.content[0].text);
```

## Example 2: Streaming Responses

Track streaming responses from Claude:

```javascript
const stream = await observa.track(
  {
    query: "Explain quantum computing",
    conversationId: "conv-123",
    messageIndex: 1,
    metadata: {
      model: "claude-3-opus-20240229",
      stream: true,
    },
  },
  async () => {
    return await anthropic.messages.stream({
      model: 'claude-3-opus-20240229',
      max_tokens: 1024,
      messages: [
        { role: 'user', content: 'Explain quantum computing' }
      ],
    });
  }
);

let fullResponse = '';
for await (const chunk of stream) {
  if (chunk.type === 'content_block_delta' && chunk.delta.type === 'text_delta') {
    const text = chunk.delta.text;
    fullResponse += text;
    process.stdout.write(text);
  }
}
```

## Example 3: Multi-Turn Conversation

Track a conversation with Claude:

```javascript
const conversationId = `conv-${Date.now()}`;
let messageIndex = 0;
const conversationHistory = [];

async function sendMessage(message) {
  messageIndex++;
  conversationHistory.push({ role: 'user', content: message });
  
  const response = await observa.track(
    {
      query: message,
      conversationId: conversationId,
      messageIndex: messageIndex,
      context: conversationHistory.slice(0, -1).map(m => m.content).join('\n'),
    },
    async () => {
      return await anthropic.messages.create({
        model: 'claude-3-opus-20240229',
        max_tokens: 1024,
        messages: conversationHistory,
      });
    }
  );
  
  const assistantMessage = response.content[0].text;
  conversationHistory.push({ role: 'assistant', content: assistantMessage });
  
  return assistantMessage;
}

// First message
const response1 = await sendMessage("Hello, my name is Alice");
console.log(response1);

// Follow-up message
const response2 = await sendMessage("What's my name?");
console.log(response2);
```

## Example 4: With System Prompt

Track calls with system prompts:

```javascript
const response = await observa.track(
  {
    query: "Write a haiku about programming",
    conversationId: "conv-123",
    messageIndex: 1,
    metadata: {
      hasSystemPrompt: true,
    },
  },
  async () => {
    return await anthropic.messages.create({
      model: 'claude-3-opus-20240229',
      max_tokens: 1024,
      system: "You are a helpful assistant that writes poetry.",
      messages: [
        { role: 'user', content: 'Write a haiku about programming' }
      ],
    });
  }
);

console.log(response.content[0].text);
```

## Example 5: Tool Use (Function Calling)

Track Claude's tool use feature:

```javascript
const tools = [
  {
    name: "get_weather",
    description: "Get the current weather in a location",
    input_schema: {
      type: "object",
      properties: {
        location: {
          type: "string",
          description: "The city and state, e.g. San Francisco, CA",
        },
      },
      required: ["location"],
    },
  },
];

async function getWeather(location) {
  if (location === "Berlin, Germany") {
    return "20Â°C, sunny";
  }
  return "Weather unknown";
}

const response = await observa.track(
  {
    query: "What's the weather in Berlin?",
    conversationId: "conv-123",
    messageIndex: 1,
    metadata: {
      tools: tools.map(t => t.name),
    },
  },
  async () => {
    return await anthropic.messages.create({
      model: 'claude-3-opus-20240229',
      max_tokens: 1024,
      tools: tools,
      messages: [
        { role: 'user', content: "What's the weather in Berlin?" }
      ],
    });
  }
);

// Handle tool use
if (response.stop_reason === 'tool_use') {
  const toolUse = response.content.find(item => item.type === 'tool_use');
  if (toolUse && toolUse.name === 'get_weather') {
    const location = toolUse.input.location;
    const weather = await getWeather(location);
    console.log(weather);
  }
}
```

## Example 6: With Custom Metadata

Add custom metadata for tracking:

```javascript
const response = await observa.track(
  {
    query: "Generate a marketing email",
    conversationId: "conv-123",
    messageIndex: 1,
    userId: "user-456",
    metadata: {
      feature: "email_generator",
      campaignId: "campaign-789",
      model: "claude-3-opus-20240229",
      version: "1.0.0",
    },
  },
  async () => {
    return await anthropic.messages.create({
      model: 'claude-3-opus-20240229',
      max_tokens: 2048,
      messages: [
        { role: 'user', content: 'Generate a marketing email' }
      ],
    });
  }
);
```

## Best Practices

1. **Use Conversation IDs**: Group related messages together
2. **Track Token Usage**: Monitor Claude's token usage
3. **Handle Tool Use**: Properly handle tool use responses
4. **Include Context**: Add conversation history for multi-turn conversations
5. **Set Appropriate Max Tokens**: Configure max_tokens based on your needs

## View Your Traces

After running these examples:

1. Go to [Dashboard](https://observa-app.vercel.app/dashboard)
2. Navigate to **Traces**
3. Filter by model to see Claude calls
4. Click on a trace for detailed information

## Next Steps

- Read the [SDK Reference](../sdk/tracking) for complete API documentation
- Check [Multi-Turn Conversations](./multi-turn-conversations) for advanced patterns
- See [Function Calling](./function-calling) for tool usage examples
- Explore [OpenAI Integration](./openai-integration) for comparison
